# This is Jordan's brain
## 概述

好吧，这个就是Jordan's brain了，也就是Jordan最核心的部分，
主要就是用强化学习算法来让Jordan学习如何play Gomoku，嗯，感觉
自己其实只做了一点微小的工作，毕竟没做啥原创性工作，嗯，大致说一下吧
算法上主要就是蒙特卡洛树搜索和卷积神经网络CNN，简单说下蒙特卡洛树搜索
这个包括四个part，选择，拓展，模拟和反向传播，简而言之在Gomoku当中
这四个part就是寻找当前状态可以达到的状态，然后拓展到（DFS）这些状态（主要是那些胜率较高的状态
，如果低于某个阙值，实际上就将该状态剪枝了），接着反复执行，直到达到目标状态
（就是出现五个子连在一起的状态）此时先更新这一状态的胜率，然后反向传播至所有经历过的装态
然后学习的过程其实就是使用当前模型不断地用蒙特卡洛搜索自己和自己下，产生了大量数据，数据就是局面和对应的胜率
然后把数据输入CNN进行拟合就好了，CNN的结构是一个5层的网络，输入层是9x9x4，其中前两个9顾名思义，后面一个4指的是最近4步的局势，
然后两层卷积层是做了两次3*3的卷积，后面再加一个全连接层，最后CNN的输出就是当前局面下的胜率或者说score，简单来说就是这样把，其实
网络结构也不是很复杂，自己做的事情其实就是再原始的别人写的8x8的模型的基础上改了下模型，
然后蒙特卡洛搜索直接用的别人代码。。。。

## 文件简述

- game文件是五子棋游戏的主题，维护了一个Board类，用来存放当前游戏局势的信息，
并且给出了落子和得到当前状态以及判断输赢的方法，还维护了一个Game类，主要维护了
两个方法一个是self_play即模型的自我对弈，start_play即与人进行对战
- human_play文件当中维护了一个Human类，用于读取在游戏中玩家的操作，主要就是
一个get_action方法，通过字符串匹配输出的x,y形式的字符串获取[x,y]坐标。
//感觉再改图像识别输入后可以直接对get_action进行重写，简单的，说就是可以让摄像头
再监控玩家下棋的时候进入一个while循环的忙等待，每隔10s读取一次图片匹配是否落子，如果
落子了则break，并且返回落子的坐标
- mcts_alphaZero 这里是强化学习的主要part之一，主要维护了模型所用的蒙特卡洛搜索树
其中node类维护了树的节点以及选择、拓展、模拟、反向传播这几个基本操作的方法，MCTS类维护了
整颗树，其中_playout 方法是从根结点往子节点进行蒙特卡洛搜索的过程，get_move_probs方法
返回了执行蒙特卡洛搜索到子节点之后得到的最终回报，update_with_move方法就是根据前述move做
了一下反向传播，更新状态转移的回报（上述为强化学习过程），最后还维护了MCTSPlayer类维护了
机器人玩家，其在执行get_action的过程中，便是寻找能得到最优回报的转换action
- mcts_pure 一个纯的蒙特卡索搜索树没有强化学习过程，主要是测试用
- policy_value_net_tensorflow 文件，存放了深度学习模型就一个做了三次卷积的5层的CNN
输入是9*9*4 就是4步之内的局势，输出是胜率或者说score
- train 执行训练任务，通过不断地让AI用当前最优模型下做MCTS自我对弈，来产生数据，再将数据输入CNN
进行拟合。
## 运行

跑着玩的话直接 python main_start.py就好拉。。。

## 惯例吐槽

图像处理那块感觉是要用我写的那个ImageProcess里面的垃圾了。。。

//常元昊求求你快开始把

## 附

参考：https://github.com/junxiaosong/AlphaZero_Gomoku
