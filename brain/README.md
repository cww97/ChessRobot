# This is Jordan's brain
### HY
## 概述
通过同CNN实现策略价值网络，用强化学习的方法来拟合得到最优的action，
以此作为模型的输出

### 策略价值网络
这里的策略价值网络指的是在给定当前局面 s 的情况下，
返回当前局面下每一个可行action的概率以及当前局面评分的模型，整体结构类似于
一个蒙特卡洛搜索树，而在收集数据对策略价值网络进行训练的过程中，也是包括了
选择，拓展，模拟和反向传播这四个步骤。

### 强化学习方法
始终由最有模型来生成数据，在生成的数据的基础上，拟合策略价值网络，使得由策略价值
网络产生的最优action逐渐接近真正的最优action

### 深度神经网络
最开始是公共的3层全卷积网络，分别使用32、64和128个 3 x 3 的filter，使用ReLu激活函数。然后再分成policy和value两个输出，
在policy这一端，先使用4个 1 x 1 的filter进行降维，再接一个全连接层，使用softmax非线性函数直接输出棋盘上每个位置的落子概率；在value这一端，
先使用2个 1 x 1 的filter进行降维，再接一个64个神经元的全连接层，
最后再接一个全连接层，使用tanh非线性函数直接输出[-1,1] 之间的局面评分。整个策略价值网络的深度只有5~6层

- 主要使用了开源的8*8的四子棋的模型，在此基础上进行了一定的修改以此来符合9*9的五子棋
## 文件简述

- game文件是五子棋游戏的主题，维护了一个Board类，用来存放当前游戏局势的信息，
并且给出了落子和得到当前状态以及判断输赢的方法，还维护了一个Game类，主要维护了
两个方法一个是self_play即模型的自我对弈，start_play即与人进行对战
- human_play文件当中维护了一个Human类，用于读取在游戏中玩家的操作，主要就是
一个get_action方法，通过字符串匹配输出的x,y形式的字符串获取[x,y]坐标。
//感觉再改图像识别输入后可以直接对get_action进行重写，简单的，说就是可以让摄像头
再监控玩家下棋的时候进入一个while循环的忙等待，每隔10s读取一次图片匹配是否落子，如果
落子了则break，并且返回落子的坐标
- mcts_alphaZero 这里是强化学习的主要part之一，主要维护了模型所用的类似蒙特卡洛搜索树的策略价值网络
其中node类维护了树的节点以及选择、拓展、模拟、反向传播这几个基本操作的方法，MCTS类维护了
整颗树，其中_playout 方法是从根结点往子节点进行蒙特卡洛搜索的过程，get_move_probs方法
返回了执行蒙特卡洛搜索到子节点之后得到的最终回报，update_with_move方法就是根据前述move做
了一下反向传播，更新状态转移的回报（上述为强化学习过程），最后还维护了MCTSPlayer类维护了
机器人玩家，其在执行get_action的过程中，便是寻找能得到最优回报的转换action
- mcts_pure 一个纯的蒙特卡索搜索树没有强化学习过程，主要是测试用
- policy_value_net_tensorflow 文件，存放了深度学习模型就一个做了三次卷积的5层的CNN
输入是9*9*4 就是4步之内的局势，输出是胜率或者说score
- train 执行训练任务，通过不断地让AI用当前最优模型下做MCTS自我对弈，来产生数据，再将数据输入CNN
进行拟合。
## 运行

跑着玩的话直接 python game.py就好拉。。。


## 附

参考：https://github.com/junxiaosong/AlphaZero_Gomoku
